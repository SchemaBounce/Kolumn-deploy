# =============================================================================
# AUTONOMOUS COLUMN PROPAGATION EXAMPLE
# =============================================================================
# This example demonstrates the power of Kolumn's autonomous column management:
# 1. Discover table schema from PostgreSQL (the SOURCE)
# 2. Use discovered columns in SQL files, Python DAGs, and Kafka streams
# 3. Create related tables that reference the source schema
# 4. Any changes to the source table columns automatically propagate everywhere
# 
# KEY CONCEPT: Change a column in "users" table ‚Üí changes everywhere seamlessly!
# =============================================================================

# -----------------------------------------------------------------------------
# STEP 1: DISCOVER SOURCE TABLE SCHEMA (The Single Source of Truth)
# -----------------------------------------------------------------------------

# Discover the existing "users" table from PostgreSQL
# This becomes our SINGLE SOURCE OF TRUTH for column definitions
discover "postgres_table" "source_users" {
  database_provider = provider.postgres
  table_name        = "users"
  schema_name       = "public"
}

# -----------------------------------------------------------------------------
# STEP 2: CREATE KOLUMN DATA OBJECT FROM DISCOVERED SCHEMA
# -----------------------------------------------------------------------------

# Convert discovered table into a Kolumn data object for reuse
# This allows us to reference ${kolumn_data_object.user_schema.columns} everywhere
create "kolumn_data_object" "user_schema" {
  name        = "User Schema"
  description = "Universal user schema derived from PostgreSQL discovery"

  # ‚ö° AUTONOMOUS MAGIC: Dynamically inherit ALL columns from discovered table
  dynamic "column" {
    for_each = discover.postgres_table.source_users.columns
    content {
      name     = column.key
      type     = column.value.type
      nullable = column.value.nullable
      default  = column.value.default

      # üõ°Ô∏è SECURITY: Auto-classify sensitive columns  
      classifications = contains(["email", "phone", "ssn"], column.key) ? [
        kolumn_classification.pii
        ] : [
        kolumn_classification.public
      ]
    }
  }

  # üìä METADATA: Track the source for governance
  metadata = {
    source           = "postgres.public.users"
    discovered_at    = timestamp()
    governance_level = "enterprise"
  }
}

# -----------------------------------------------------------------------------
# STEP 3: CREATE CLASSIFICATIONS FOR DATA GOVERNANCE
# -----------------------------------------------------------------------------

create "kolumn_classification" "pii" {
  name        = "PII"
  description = "Personally Identifiable Information"
  requirements = {
    encryption       = true
    audit_access     = true
    retention_period = "7_years"
  }
}

create "kolumn_classification" "public" {
  name        = "Public"
  description = "Public information with no restrictions"
  requirements = {
    encryption   = false
    audit_access = false
  }
}

# -----------------------------------------------------------------------------
# STEP 4: DISCOVER AND INTERPOLATE SQL FILES
# -----------------------------------------------------------------------------

# Discover SQL analytics file that uses our user schema
discover "kolumn_file" "user_analytics_sql" {
  location = "./files/user_analytics.sql"
  inputs = {
    # üîó AUTONOMOUS PROPAGATION: SQL file automatically gets latest column schema
    user_columns       = kolumn_data_object.user_schema.columns
    user_table         = "users"
    classification_pii = kolumn_classification.pii
  }

  # ‚ö° BIDIRECTIONAL: Extract SQL objects FROM the file
  extractor_config = {
    tables    = "true"
    views     = "true"
    functions = "true"
    indexes   = "true"
  }
}

# Discover Python DAG file that processes user data
discover "kolumn_file" "user_processing_dag" {
  location = "./files/user_processing_dag.py"
  inputs = {
    # üîó AUTONOMOUS PROPAGATION: Python DAG gets same column schema automatically
    user_schema    = kolumn_data_object.user_schema.columns
    kafka_topic    = kafka_topic.user_events.name
    postgres_table = postgres_table.user_analytics.name
  }
}

# -----------------------------------------------------------------------------
# STEP 5: CREATE RELATED TABLES THAT REFERENCE THE SOURCE SCHEMA
# -----------------------------------------------------------------------------

# Analytics table that inherits structure from source but adds computed columns
create "postgres_table" "user_analytics" {
  # ‚ö° AUTONOMOUS MAGIC: Inherits ALL columns from the source user schema
  dynamic "column" {
    for_each = kolumn_data_object.user_schema.columns
    content {
      name     = column.value.name
      type     = column.value.type
      nullable = column.value.nullable

      # üõ°Ô∏è SECURITY: Auto-apply encryption based on classification
      encrypted = contains(column.value.classifications, kolumn_classification.pii)
    }
  }

  # Add analytics-specific columns
  column "first_login" {
    type     = "TIMESTAMP"
    nullable = true
  }

  column "last_activity" {
    type     = "TIMESTAMP"
    nullable = true
  }

  column "total_orders" {
    type    = "INTEGER"
    default = 0
  }

  # üìä METADATA: Link back to source for traceability
  metadata = {
    source_schema     = kolumn_data_object.user_schema.name
    purpose           = "User behavior analytics"
    refresh_frequency = "hourly"
  }
}

# Archive table for historical user data (same schema, different retention)
create "postgres_table" "user_archive" {
  # ‚ö° AUTONOMOUS MAGIC: Exact same columns as source user schema
  dynamic "column" {
    for_each = kolumn_data_object.user_schema.columns
    content {
      name     = column.value.name
      type     = column.value.type
      nullable = column.value.nullable
    }
  }

  # Archive-specific metadata
  column "archived_at" {
    type    = "TIMESTAMP"
    default = "CURRENT_TIMESTAMP"
  }

  metadata = {
    source_schema    = kolumn_data_object.user_schema.name
    purpose          = "Historical user data archive"
    retention_policy = "10_years"
  }
}

# -----------------------------------------------------------------------------
# STEP 6: CREATE KAFKA STREAMS THAT USE THE SAME SCHEMA
# -----------------------------------------------------------------------------

# Kafka topic for user events with schema derived from our data object
create "kafka_topic" "user_events" {
  name               = "user-events"
  partitions         = 6
  replication_factor = 3

  # ‚ö° AUTONOMOUS MAGIC: Schema registry automatically uses our user schema
  schema_registry_config = {
    # Add event-specific fields
    fields = {
      event_type = {
        type     = "string"
        nullable = false
      }
      event_timestamp = {
        type     = "long"
        nullable = false
      }
    }
  }

  # Dynamic fields from data object
  dynamic "schema_field" {
    for_each = kolumn_data_object.user_schema.columns
    content {
      name     = schema_field.value.name
      type     = schema_field.value.type == "VARCHAR" ? "string" : schema_field.value.type == "INTEGER" ? "int" : schema_field.value.type == "TIMESTAMP" ? "long" : "string"
      nullable = schema_field.value.nullable
    }
  }

  metadata = {
    source_schema = kolumn_data_object.user_schema.name
    purpose       = "Real-time user events"
  }
}

# -----------------------------------------------------------------------------  
# STEP 7: CREATE ORCHESTRATION JOBS THAT PROCESS THE DATA
# -----------------------------------------------------------------------------

# Dagster job that processes user data using our schema
create "dagster_job" "user_data_pipeline" {
  name        = "user-data-pipeline"
  description = "Process user data from source to analytics"

  # üîó AUTONOMOUS PROPAGATION: Pipeline configuration uses our schema
  config = {
    # Source and target table schemas automatically in sync
    source_table = {
      name    = "users"
      columns = [for col in kolumn_data_object.user_schema.columns : col.name]
    }

    target_table = {
      name    = postgres_table.user_analytics.name
      columns = [for col in kolumn_data_object.user_schema.columns : col.name]
    }

    kafka_topic = kafka_topic.user_events.name

    # üõ°Ô∏è SECURITY: PII handling based on classifications
    pii_columns = [
      for col in kolumn_data_object.user_schema.columns : col.name
      if contains(col.classifications, kolumn_classification.pii)
    ]
  }

  metadata = {
    source_schema = kolumn_data_object.user_schema.name
    schedule      = "0 * * * *" # Every hour
  }
}

# -----------------------------------------------------------------------------
# STEP 8: DEMONSTRATE THE AUTONOMOUS MAGIC
# -----------------------------------------------------------------------------

# üéØ THE MAGIC: When you change the source PostgreSQL table:
# 1. Run: ALTER TABLE users ADD COLUMN middle_name VARCHAR(100);
# 2. Run: kolumn plan
# 3. Kolumn will detect the schema change and update:
#    - ‚úÖ kolumn_data_object.user_schema (gets new column automatically)
#    - ‚úÖ postgres_table.user_analytics (gets middle_name column)
#    - ‚úÖ postgres_table.user_archive (gets middle_name column)  
#    - ‚úÖ kafka_topic.user_events (schema registry updated)
#    - ‚úÖ dagster_job.user_data_pipeline (config updated)
#    - ‚úÖ SQL files get new column available via ${user_columns}
#    - ‚úÖ Python DAGs get updated schema via ${user_schema}
#
# üîÑ ZERO CONFIGURATION REQUIRED - Everything stays in perfect sync!

# -----------------------------------------------------------------------------
# STEP 9: VALIDATION AND MONITORING
# -----------------------------------------------------------------------------

# Create validation rules to ensure schema consistency
create "kolumn_validation" "schema_consistency" {
  name        = "User Schema Consistency Check"
  description = "Validates that all user-related resources stay in sync"

  rules = [
    {
      type   = "schema_match"
      source = discover.postgres_table.source_users
      targets = [
        postgres_table.user_analytics,
        postgres_table.user_archive,
        kafka_topic.user_events
      ]
      severity = "error"
    },

    {
      type              = "classification_coverage"
      data_object       = kolumn_data_object.user_schema
      required_coverage = 100
      severity          = "warning"
    }
  ]
}

# Create monitoring for the autonomous propagation system
create "kolumn_monitor" "schema_drift" {
  name        = "User Schema Drift Detection"
  description = "Monitors for schema changes and propagation status"

  checks = [
    {
      type           = "schema_drift"
      source         = discover.postgres_table.source_users
      frequency      = "5m"
      alert_on_drift = true
    },

    {
      type                  = "propagation_status"
      data_object           = kolumn_data_object.user_schema
      frequency             = "1m"
      max_propagation_delay = "30s"
    }
  ]

  notifications = {
    slack_channel = "#data-engineering"
    email_alerts  = ["team-lead@company.com"]
  }
}

# =============================================================================
# AUTONOMOUS PROPAGATION SUMMARY
# =============================================================================
# 
# This example demonstrates Kolumn's most powerful feature:
# 
# üéØ SINGLE SOURCE OF TRUTH
# ‚Ä¢ PostgreSQL "users" table is discovered as the authoritative schema
# ‚Ä¢ All other resources inherit from this single source
# 
# ‚ö° AUTOMATIC PROPAGATION  
# ‚Ä¢ Add column ‚Üí Detected in source table via discovery
# ‚Ä¢ Update propagates ‚Üí Data object, related tables, Kafka, DAGs, files
# ‚Ä¢ Zero manual updates ‚Üí Everything stays perfectly synchronized
# 
# üõ°Ô∏è BUILT-IN GOVERNANCE
# ‚Ä¢ Classification system ‚Üí Automatically applied based on column names
# ‚Ä¢ Security policies ‚Üí Inherited across all resources
# ‚Ä¢ Compliance tracking ‚Üí Full lineage from source to all derivatives
# 
# üîÑ OPERATIONAL EXCELLENCE
# ‚Ä¢ Drift detection ‚Üí Monitors for schema changes continuously  
# ‚Ä¢ Validation rules ‚Üí Ensures consistency across all resources
# ‚Ä¢ Alerting system ‚Üí Notifies team of any propagation issues
# 
# üí° THE RESULT
# ‚Ä¢ Developers work with ONE schema definition
# ‚Ä¢ Changes propagate everywhere automatically
# ‚Ä¢ Zero configuration drift between systems
# ‚Ä¢ Complete data lineage and governance
# 
# This is Infrastructure-as-Code evolved into Schema-as-Code! üöÄ
# =============================================================================