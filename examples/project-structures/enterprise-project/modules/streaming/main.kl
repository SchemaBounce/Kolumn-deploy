# Streaming Module - Auto-discovered from modules/streaming/
# Provides enterprise streaming data platform with Kafka, real-time processing, and stream analytics

# === MODULE VARIABLES ===
variable "environment" {
  description = "Environment name"
  type        = string
}

variable "project_name" {
  description = "Project name for resource naming"
  type        = string
}

variable "kafka_config" {
  description = "Kafka configuration from parent"
  type        = any
}

variable "feature_flags" {
  description = "Feature flags for streaming capabilities"
  type        = any
}

# === KAFKA TOPICS FOR DATA STREAMING ===

create "kafka_topic" "customer_events" {
  name               = "customer-events-${var.environment}"
  partitions         = var.environment == "production" ? 24 : 6
  replication_factor = var.environment == "production" ? 3 : 1

  config = {
    "retention.ms"        = "604800000" # 7 days
    "cleanup.policy"      = "delete"
    "compression.type"    = "lz4"
    "max.message.bytes"   = "1048576" # 1MB
    "min.insync.replicas" = var.environment == "production" ? "2" : "1"
  }

  schema = {
    key_schema = "customer_id:STRING"
    value_schema = {
      customer_id   = "STRING"
      event_type    = "STRING"
      event_data    = "MAP<STRING,STRING>"
      timestamp     = "TIMESTAMP"
      source_system = "STRING"
      version       = "INT"
    }
  }
}

create "kafka_topic" "transaction_stream" {
  name               = "transactions-${var.environment}"
  partitions         = var.environment == "production" ? 48 : 12
  replication_factor = var.environment == "production" ? 3 : 1

  config = {
    "retention.ms"        = "86400000" # 24 hours (financial data)
    "cleanup.policy"      = "delete"
    "compression.type"    = "snappy"
    "max.message.bytes"   = "524288" # 512KB
    "min.insync.replicas" = var.environment == "production" ? "3" : "1"
  }

  schema = {
    key_schema = "transaction_id:STRING"
    value_schema = {
      transaction_id   = "STRING"
      customer_id      = "STRING"
      amount           = "DECIMAL"
      currency         = "STRING"
      transaction_type = "STRING"
      payment_method   = "STRING"
      merchant_id      = "STRING"
      timestamp        = "TIMESTAMP"
      risk_score       = "DOUBLE"
      metadata         = "MAP<STRING,STRING>"
    }
  }
}

create "kafka_topic" "data_quality_alerts" {
  name               = "data-quality-alerts-${var.environment}"
  partitions         = var.environment == "production" ? 12 : 3
  replication_factor = var.environment == "production" ? 3 : 1

  config = {
    "retention.ms"        = "2592000000" # 30 days
    "cleanup.policy"      = "delete"
    "compression.type"    = "gzip"
    "max.message.bytes"   = "262144" # 256KB
    "min.insync.replicas" = var.environment == "production" ? "2" : "1"
  }

  schema = {
    key_schema = "alert_id:STRING"
    value_schema = {
      alert_id     = "STRING"
      alert_type   = "STRING"
      severity     = "STRING"
      source_topic = "STRING"
      source_table = "STRING"
      description  = "STRING"
      metrics      = "MAP<STRING,DOUBLE>"
      timestamp    = "TIMESTAMP"
      resolved     = "BOOLEAN"
    }
  }
}

# === STREAMING ANALYTICS ===

create "kafka_stream" "real_time_customer_enrichment" {
  name = "customer-enrichment-${var.environment}"

  input_topics = [kafka_topic.customer_events]
  output_topic = "enriched-customers-${var.environment}"

  processing_config = {
    application_id       = "customer-enrichment-${var.environment}"
    processing_guarantee = "exactly_once_v2"
    num_stream_threads   = var.environment == "production" ? 8 : 2

    # Windowing for sessionization
    windowing = {
      type                   = "session"
      inactivity_gap_minutes = 30
    }
  }

  # Stream processing logic
  processing_topology = {
    # Customer event enrichment with lookup tables
    enrichment_stores = [
      {
        name            = "customer_profiles"
        type            = "key_value"
        changelog_topic = "customer-profiles-changelog-${var.environment}"
      },
      {
        name            = "product_catalog"
        type            = "key_value"
        changelog_topic = "products-changelog-${var.environment}"
      }
    ]

    transformations = [
      {
        type      = "filter"
        condition = "event_type != 'heartbeat'"
      },
      {
        type         = "enrich"
        lookup_store = "customer_profiles"
        lookup_key   = "customer_id"
      },
      {
        type         = "aggregate"
        grouping_key = "customer_id"
        window_type  = "session"
        aggregations = ["count", "sum", "avg"]
      }
    ]
  }
}

create "kafka_stream" "fraud_detection_pipeline" {
  name = "fraud-detection-${var.environment}"

  input_topics = [kafka_topic.transaction_stream]
  output_topic = "fraud-alerts-${var.environment}"

  processing_config = {
    application_id       = "fraud-detection-${var.environment}"
    processing_guarantee = "exactly_once_v2"
    num_stream_threads   = var.environment == "production" ? 12 : 3

    # Real-time windowing for fraud patterns
    windowing = {
      type             = "tumbling"
      duration_minutes = 5
    }
  }

  # Advanced fraud detection logic
  processing_topology = {
    # Machine learning model integration
    ml_models = [
      {
        name       = "transaction_anomaly_detector"
        type       = "isolation_forest"
        model_path = "s3://ml-models/fraud/isolation_forest_v2.pkl"
        threshold  = 0.7
      },
      {
        name = "velocity_checker"
        type = "rule_based"
        rules = [
          "transaction_count_per_minute > 10",
          "amount_deviation > 3_std_dev",
          "new_merchant_risk > 0.8"
        ]
      }
    ]

    enrichment_stores = [
      {
        name            = "customer_risk_profiles"
        type            = "key_value"
        changelog_topic = "customer-risk-changelog-${var.environment}"
      },
      {
        name            = "merchant_reputation"
        type            = "key_value"
        changelog_topic = "merchant-reputation-changelog-${var.environment}"
      }
    ]

    transformations = [
      {
        type         = "enrich"
        lookup_store = "customer_risk_profiles"
        lookup_key   = "customer_id"
      },
      {
        type         = "ml_predict"
        model        = "transaction_anomaly_detector"
        input_fields = ["amount", "merchant_id", "transaction_type"]
        output_field = "anomaly_score"
      },
      {
        type         = "rule_evaluate"
        model        = "velocity_checker"
        output_field = "velocity_risk"
      },
      {
        type         = "aggregate"
        grouping_key = "customer_id"
        window_type  = "tumbling"
        aggregations = ["transaction_count", "total_amount", "unique_merchants"]
      },
      {
        type      = "filter"
        condition = "anomaly_score > 0.7 OR velocity_risk > 0.8"
      }
    ]
  }

  # Enable only if feature flag is set
  enabled = var.feature_flags.enable_real_time_processing
}

# === DATA QUALITY MONITORING ===

create "kafka_stream" "data_quality_monitor" {
  name = "data-quality-monitor-${var.environment}"

  input_topics = [
    kafka_topic.customer_events,
    kafka_topic.transaction_stream
  ]
  output_topic = kafka_topic.data_quality_alerts.name

  processing_config = {
    application_id       = "data-quality-monitor-${var.environment}"
    processing_guarantee = "at_least_once" # Lower guarantee for monitoring
    num_stream_threads   = var.environment == "production" ? 4 : 1
  }

  # Data quality checks
  quality_checks = {
    schema_validation = {
      strict_mode     = var.environment == "production"
      required_fields = ["timestamp", "source_system"]
    }

    freshness_checks = {
      max_delay_minutes = 30
      alert_threshold   = 0.95 # Alert if 95% of messages are delayed
    }

    completeness_checks = {
      null_tolerance  = 0.05 # 5% null values allowed
      critical_fields = ["customer_id", "amount"]
    }

    accuracy_checks = {
      range_validations = {
        "amount"     = { min = 0.01, max = 100000.00 }
        "risk_score" = { min = 0.0, max = 1.0 }
      }
      format_validations = {
        "currency"    = "^[A-Z]{3}$"      # ISO currency codes
        "customer_id" = "^[0-9a-f-]{36}$" # UUID format
      }
    }
  }
}

# === KAFKA CONNECT FOR DATA INTEGRATION ===

create "kafka_connector" "postgres_source_connector" {
  name            = "postgres-source-${var.environment}"
  type            = "source"
  connector_class = "io.debezium.connector.postgresql.PostgresConnector"

  config = {
    # Database connection
    "database.hostname"    = var.postgres_config.host
    "database.port"        = var.postgres_config.port
    "database.user"        = var.postgres_config.username
    "database.password"    = var.postgres_config.password
    "database.dbname"      = var.postgres_config.database
    "database.server.name" = "${var.project_name}-${var.environment}"

    # CDC configuration
    "table.include.list" = "public.customers,public.transactions,public.products"
    "plugin.name"        = "pgoutput"
    "slot.name"          = "kolumn_${var.environment}_slot"
    "publication.name"   = "kolumn_${var.environment}_pub"

    # Change event routing
    "transforms"                   = "route,extractNewRecordState"
    "transforms.route.type"        = "org.apache.kafka.connect.transforms.RegexRouter"
    "transforms.route.regex"       = "([^.]+)\\.([^.]+)\\.([^.]+)"
    "transforms.route.replacement" = "$3-cdc-${var.environment}"

    "transforms.extractNewRecordState.type"                 = "io.debezium.transforms.ExtractNewRecordState"
    "transforms.extractNewRecordState.drop.tombstones"      = "false"
    "transforms.extractNewRecordState.delete.handling.mode" = "rewrite"
  }

  # Error handling and monitoring
  error_handling = {
    "errors.tolerance"            = "all"
    "errors.log.enable"           = "true"
    "errors.log.include.messages" = "true"
  }
}

create "kafka_connector" "snowflake_sink_connector" {
  name            = "snowflake-sink-${var.environment}"
  type            = "sink"
  connector_class = "com.snowflake.kafka.connector.SnowflakeSinkConnector"

  config = {
    # Snowflake connection
    "snowflake.url.name"      = "https://${var.snowflake_config.account}.snowflakecomputing.com"
    "snowflake.user.name"     = var.snowflake_config.username
    "snowflake.private.key"   = var.snowflake_config.password
    "snowflake.database.name" = var.snowflake_config.database
    "snowflake.schema.name"   = "STREAMING_INGESTION"

    # Topic to table mapping
    "topics" = join(",", [
      "enriched-customers-${var.environment}",
      "fraud-alerts-${var.environment}",
      kafka_topic.transaction_stream.name,
      kafka_topic.data_quality_alerts.name
    ])

    # Data format and transformation
    "key.converter"                  = "org.apache.kafka.connect.storage.StringConverter"
    "value.converter"                = "org.apache.kafka.connect.json.JsonConverter"
    "value.converter.schemas.enable" = "false"

    # Snowflake-specific settings
    "buffer.count.records" = var.environment == "production" ? "10000" : "1000"
    "buffer.flush.time"    = "60"      # seconds
    "buffer.size.bytes"    = "5000000" # 5MB

    # Performance optimization
    "snowflake.ingestion.method"      = "SNOWPIPE_STREAMING"
    "snowflake.enable.schematization" = "true"
  }
}

# === SCHEMA REGISTRY CONFIGURATION ===

create "schema_registry_subject" "customer_events_value_schema" {
  subject     = "${kafka_topic.customer_events.name}-value"
  schema_type = "AVRO"

  schema = jsonencode({
    type      = "record"
    name      = "CustomerEvent"
    namespace = "${var.project_name}.events"
    fields = [
      { name = "customer_id", type = "string" },
      { name = "event_type", type = "string" },
      { name = "event_data", type = { type = "map", values = "string" } },
      { name = "timestamp", type = { type = "long", logicalType = "timestamp-millis" } },
      { name = "source_system", type = "string" },
      { name = "version", type = "int", default = 1 }
    ]
  })

  compatibility_level = "BACKWARD"
}

create "schema_registry_subject" "transaction_stream_value_schema" {
  subject     = "${kafka_topic.transaction_stream.name}-value"
  schema_type = "AVRO"

  schema = jsonencode({
    type      = "record"
    name      = "Transaction"
    namespace = "${var.project_name}.financial"
    fields = [
      { name = "transaction_id", type = "string" },
      { name = "customer_id", type = "string" },
      { name = "amount", type = { type = "bytes", logicalType = "decimal", precision = 15, scale = 2 } },
      { name = "currency", type = "string", default = "USD" },
      { name = "transaction_type", type = { type = "enum", name = "TransactionType", symbols = ["PURCHASE", "REFUND", "TRANSFER", "PAYMENT"] } },
      { name = "payment_method", type = "string" },
      { name = "merchant_id", type = ["null", "string"], default = null },
      { name = "timestamp", type = { type = "long", logicalType = "timestamp-millis" } },
      { name = "risk_score", type = "double", default = 0.0 },
      { name = "metadata", type = { type = "map", values = "string" }, default = {} }
    ]
  })

  compatibility_level = "FORWARD" # Allow adding new fields
}

# === MODULE OUTPUTS ===

output "streaming_topics" {
  description = "Kafka topics for streaming data"
  value = {
    customer_events = {
      name            = kafka_topic.customer_events.name
      partitions      = kafka_topic.customer_events.partitions
      retention_hours = kafka_topic.customer_events.config["retention.ms"] / (1000 * 60 * 60)
    }
    transaction_stream = {
      name            = kafka_topic.transaction_stream.name
      partitions      = kafka_topic.transaction_stream.partitions
      retention_hours = kafka_topic.transaction_stream.config["retention.ms"] / (1000 * 60 * 60)
    }
    data_quality_alerts = {
      name           = kafka_topic.data_quality_alerts.name
      partitions     = kafka_topic.data_quality_alerts.partitions
      retention_days = kafka_topic.data_quality_alerts.config["retention.ms"] / (1000 * 60 * 60 * 24)
    }
  }
}

output "streaming_applications" {
  description = "Kafka Streams applications"
  value = {
    customer_enrichment = {
      name                 = kafka_stream.real_time_customer_enrichment.name
      application_id       = kafka_stream.real_time_customer_enrichment.processing_config.application_id
      processing_guarantee = kafka_stream.real_time_customer_enrichment.processing_config.processing_guarantee
    }
    fraud_detection = {
      name      = kafka_stream.fraud_detection_pipeline.name
      enabled   = kafka_stream.fraud_detection_pipeline.enabled
      ml_models = length(kafka_stream.fraud_detection_pipeline.processing_topology.ml_models)
    }
    data_quality_monitor = {
      name                   = kafka_stream.data_quality_monitor.name
      quality_checks_enabled = length(kafka_stream.data_quality_monitor.quality_checks) > 0
    }
  }
}

output "data_connectors" {
  description = "Kafka Connect connectors for data integration"
  value = {
    postgres_source = {
      name             = kafka_connector.postgres_source_connector.name
      type             = kafka_connector.postgres_source_connector.type
      tables_monitored = split(",", kafka_connector.postgres_source_connector.config["table.include.list"])
    }
    snowflake_sink = {
      name             = kafka_connector.snowflake_sink_connector.name
      type             = kafka_connector.snowflake_sink_connector.type
      target_schema    = kafka_connector.snowflake_sink_connector.config["snowflake.schema.name"]
      ingestion_method = kafka_connector.snowflake_sink_connector.config["snowflake.ingestion.method"]
    }
  }
}

output "schema_registry_subjects" {
  description = "Schema Registry subjects for data governance"
  value = {
    customer_events = {
      subject       = schema_registry_subject.customer_events_value_schema.subject
      compatibility = schema_registry_subject.customer_events_value_schema.compatibility_level
      schema_type   = schema_registry_subject.customer_events_value_schema.schema_type
    }
    transaction_stream = {
      subject       = schema_registry_subject.transaction_stream_value_schema.subject
      compatibility = schema_registry_subject.transaction_stream_value_schema.compatibility_level
      schema_type   = schema_registry_subject.transaction_stream_value_schema.schema_type
    }
  }
}