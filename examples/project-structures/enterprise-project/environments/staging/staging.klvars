# Staging Environment Variables
# Production-like configuration with reduced scale for testing and validation

# === ENVIRONMENT CONFIGURATION ===
environment = "staging"
workspace = "stage"

# === SCALING AND PERFORMANCE ===
scaling_config = {
  min_instances = 2
  max_instances = 15
  target_cpu_percent = 75
  scale_up_cooldown_minutes = 3
  scale_down_cooldown_minutes = 10
  enable_predictive_scaling = false
}

# === SECURITY AND COMPLIANCE ===
compliance_level = "standard"
encryption_level = "high"

security_config = {
  enable_waf = true
  enable_ddos_protection = false
  enable_intrusion_detection = true
  enable_vulnerability_scanning = true
  enable_secrets_scanning = true
  enable_container_scanning = false
  security_group_lockdown = false
  vpn_required = false
  mfa_required = false
  session_recording_enabled = false
}

# === DATA GOVERNANCE ===
enable_audit_logging = true
enable_policy_enforcement = true

data_retention_policy = {
  pii_retention_days = 90          # 3 months for testing
  financial_retention_days = 365   # 1 year for testing
  general_retention_days = 180     # 6 months for business data
  audit_log_retention_days = 365   # 1 year for compliance testing
}

# === DATABASE CONFIGURATIONS ===
postgres_config = {
  host = "staging-postgres-cluster.internal"
  port = 5432
  database = "kolumn_staging"
  username = "kolumn_staging_user"
  password = "${env.POSTGRES_STAGING_PASSWORD}"
  
  instance_class = "db.r6g.xlarge"
  multi_az = false
  backup_retention_days = 14
  backup_window = "03:00-04:00"
  maintenance_window = "Sun:04:00-Sun:05:00"
  performance_insights_enabled = true
  monitoring_interval = 300
  
  ssl_mode = "require"
  connection_pool_size = 50
  statement_timeout = "30s"
  idle_timeout = "600s"
}

snowflake_config = {
  account = "enterprise-staging"
  username = "KOLUMN_STAGING_USER"
  password = "${env.SNOWFLAKE_STAGING_PASSWORD}"
  database = "KOLUMN_STAGING"
  warehouse = "KOLUMN_STAGING_WH"
  role = "KOLUMN_STAGING_ROLE"
  
  warehouse_size = "LARGE"
  auto_suspend = 300  # 5 minutes
  min_cluster_count = 1
  max_cluster_count = 3
  scaling_policy = "STANDARD"
}

mongodb_config = {
  cluster_name = "staging-mongodb-cluster"
  cluster_tier = "M20"  # Medium cluster for staging
  provider_instance_size = "M20"
  cloud_provider = "AWS"
  region = "us-east-1"
  
  disk_size_gb = 100
  backup_enabled = true
  auto_scaling_enabled = false
  connection_string = "${env.MONGODB_STAGING_CONNECTION_STRING}"
  
  replica_set_count = 1
  shard_count = 1
  encryption_at_rest = "AWS"
}

# === STREAMING CONFIGURATION ===
kafka_config = {
  cluster_name = "staging-streaming-platform"
  kafka_version = "3.5"
  num_brokers = 3
  broker_instance_type = "kafka.m5.large"
  storage_per_broker = 200  # GB
  
  encryption_in_transit = "TLS"
  encryption_at_rest = "AES-256"
  
  monitoring = {
    jmx_exporter_enabled = true
    cloudwatch_logs_enabled = true
    prometheus_metrics_enabled = false
  }
  
  client_authentication = "TLS_PLAINTEXT"
  configuration_overrides = {
    "auto.create.topics.enable" = "false"
    "default.replication.factor" = "3"
    "min.insync.replicas" = "2"
    "unclean.leader.election.enable" = "false"
    "log.retention.hours" = "72"  # 3 days
    "log.segment.bytes" = "536870912"  # 512MB
    "num.partitions" = "12"
  }
}

# === INFRASTRUCTURE ===
aws_config = {
  region = "us-east-1"
  availability_zones = ["us-east-1a", "us-east-1b"]
  
  vpc_cidr = "10.1.0.0/16"
  private_subnet_cidrs = ["10.1.1.0/24", "10.1.2.0/24"]
  public_subnet_cidrs = ["10.1.101.0/24", "10.1.102.0/24"]
  
  enable_nat_gateway = true
  enable_vpc_flow_logs = false
  enable_dns_hostnames = true
  enable_dns_support = true
}

# === BACKUP AND DISASTER RECOVERY ===
backup_config = {
  backup_retention_days = 14
  enable_cross_region_backup = false
  disaster_recovery_region = "us-west-2"
  backup_schedule = "0 3 * * *"  # Daily at 3 AM
  
  # RPO/RTO targets for staging
  rpo_minutes = 60   # Maximum 1 hour of data loss
  rto_minutes = 240  # Maximum 4 hours recovery time
  
  backup_encryption_enabled = true
  backup_storage_class = "STANDARD"
}

# === MONITORING AND ALERTING ===
monitoring_config = {
  alert_email = "alerts-staging@company.com"
  slack_webhook_url = "${env.SLACK_STAGING_WEBHOOK}"
  pagerduty_integration_key = "${env.PAGERDUTY_STAGING_KEY}"
  
  enable_detailed_monitoring = false
  metric_collection_interval = 300
  log_retention_days = 30
  
  critical_alert_threshold = 0.90  # 90% availability for staging
  warning_alert_threshold = 0.85
  
  dashboard_refresh_interval = "15m"
  enable_synthetic_monitoring = false
}

# === FEATURE FLAGS ===
feature_flags = {
  enable_machine_learning = true
  enable_real_time_processing = true
  enable_advanced_analytics = false
  enable_data_lineage_tracking = true
  enable_auto_scaling = false
  enable_cost_optimization = false
  enable_disaster_recovery_testing = false
}

# === PROVIDER VERSIONS ===
provider_version_constraints = {
  postgres = "~> 2.1"
  snowflake = "~> 0.95"
  kafka = "~> 3.0"
  mongodb = "~> 1.8"
  s3 = "~> 6.0"
  redis = "~> 3.2"
}

# === ORGANIZATIONAL METADATA ===
organization = "Enterprise Corp"
team = "Data Platform Engineering"
cost_center = "CC-12345"
project_name = "enterprise-data-platform"
timezone = "UTC"

# === TAGS ===
common_tags = {
  Environment = "staging"
  Project = "enterprise-data-platform"
  Organization = "Enterprise Corp"
  Team = "Data Platform Engineering"
  CostCenter = "CC-12345"
  Compliance = "standard"
  BackupRequired = "true"
  MonitoringLevel = "standard"
  DataClassification = "internal"
  ManagedBy = "kolumn"
}