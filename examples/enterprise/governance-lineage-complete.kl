# Complete Data Governance and Lineage Configuration
# Demonstrates enterprise data governance, lineage tracking, and compliance automation

# =====================================
# ENTERPRISE DATA DISCOVERY ENGINE
# =====================================

# Comprehensive AI-Powered Discovery
create "kolumn_discovery_engine" "enterprise_discovery_v2" {
  name    = "Enterprise Data Discovery Engine v2.0"
  version = "2.0"

  # Multi-modal discovery capabilities
  discovery_types = [
    "schema", "pii", "lineage", "quality",
    "relationships", "usage_patterns", "anomalies"
  ]

  # Advanced AI configuration
  ai_configuration {
    enabled = true

    # Multiple ML models for different detection types
    ml_models = {
      # PII detection models
      pii_detection    = "transformer-pii-v3.2"
      pattern_matching = "regex-neural-hybrid-v2.0"
      context_analysis = "bert-context-v1.5"

      # Data quality models
      quality_assessment = "data-quality-ensemble-v2.1"
      anomaly_detection  = "isolation-forest-v1.8"
      drift_detection    = "statistical-drift-v1.0"

      # Lineage inference models
      lineage_inference      = "graph-neural-network-v2.0"
      relationship_discovery = "similarity-clustering-v1.5"

      # Usage pattern analysis
      usage_analytics         = "time-series-forecasting-v1.2"
      access_pattern_analysis = "sequential-pattern-mining-v1.0"
    }

    # Model performance configuration
    confidence_thresholds = {
      pii_detection      = 0.92
      quality_assessment = 0.88
      lineage_inference  = 0.85
      anomaly_detection  = 0.95
    }

    # Performance optimization
    performance_optimization {
      parallel_processing = true
      gpu_acceleration    = true
      batch_size          = 2000
      max_workers         = 16

      # Memory management
      streaming_processing       = true
      memory_limit               = "8GB"
      cache_intermediate_results = true

      # Distributed processing
      distributed_training = true
      model_sharding       = true
      gradient_compression = true
    }

    # Continuous learning
    continuous_learning = {
      enabled                    = true
      feedback_loop              = true
      model_retraining_schedule  = "weekly"
      human_feedback_integration = true

      # Active learning
      active_learning      = true
      uncertainty_sampling = true
      diversity_sampling   = true
    }
  }

  # Advanced PII detection configuration
  pii_detection {
    enabled = true

    # Multi-layered detection approach
    detection_layers = [
      {
        name    = "pattern_based"
        enabled = true
        patterns = [
          "email", "phone", "ssn", "credit_card", "passport",
          "drivers_license", "ip_address", "mac_address", "iban"
        ]
        confidence_boost = 0.1
      },
      {
        name                 = "ml_based"
        enabled              = true
        model                = "transformer-pii-v3.2"
        context_window       = 256
        confidence_threshold = 0.85
      },
      {
        name                          = "semantic_analysis"
        enabled                       = true
        word_embeddings               = "fasttext-pii-v2.0"
        semantic_similarity_threshold = 0.8
      },
      {
        name                  = "statistical_analysis"
        enabled               = true
        entropy_analysis      = true
        distribution_analysis = true
        uniqueness_analysis   = true
      }
    ]

    # Custom pattern definitions
    custom_patterns = {
      # Company-specific identifiers
      employee_id    = "^EMP[0-9]{6}$"
      customer_id    = "^CUST[0-9]{8}$"
      order_id       = "^ORD[0-9]{10}$"
      transaction_id = "^TXN[0-9]{12}$"

      # Industry-specific patterns
      medicare_id     = "^[0-9]{4}-[0-9]{3}-[0-9]{4}$"
      passport_number = "^[A-Z0-9]{6,9}$"

      # Regional patterns
      uk_national_insurance = "^[A-Z]{2}[0-9]{6}[A-Z]$"
      canadian_sin          = "^[0-9]{3}-[0-9]{3}-[0-9]{3}$"
    }

    # Context-aware classification
    context_analysis = {
      enabled                    = true
      column_name_analysis       = true
      table_name_analysis        = true
      comment_analysis           = true
      data_relationship_analysis = true

      # Contextual boosting
      context_confidence_boost = {
        column_name_match      = 0.15
        table_context_match    = 0.10
        relationship_inference = 0.08
      }
    }

    # Classification automation
    auto_classification     = true
    manual_review_threshold = 0.75
    auto_approve_threshold  = 0.95

    # False positive reduction
    false_positive_reduction = {
      enabled = true
      whitelist_patterns = [
        "test_email@example.com",
        "1234567890", # Common test phone
        "000-00-0000" # Invalid SSN pattern
      ]

      # Business rule filters
      business_rules = [
        {
          rule               = "exclude_test_data"
          condition          = "table_name LIKE '%test%' OR table_name LIKE '%sample%'"
          confidence_penalty = -0.2
        },
        {
          rule               = "exclude_lookup_tables"
          condition          = "table_name LIKE '%lookup%' OR table_name LIKE '%reference%'"
          confidence_penalty = -0.1
        }
      ]
    }
  }

  # Comprehensive lineage tracking
  lineage_tracking {
    enabled = true

    # Multi-level lineage capture
    lineage_levels = [
      {
        level       = "table_level"
        enabled     = true
        capture_ddl = true
        capture_dml = true
      },
      {
        level                 = "column_level"
        enabled               = true
        track_transformations = true
        track_aggregations    = true
      },
      {
        level                = "field_level"
        enabled              = true
        kafka_field_tracking = true
        json_field_tracking  = true
      },
      {
        level                 = "application_level"
        enabled               = true
        api_call_tracking     = true
        microservice_tracking = true
      }
    ]

    # Cross-provider lineage
    cross_provider = true

    # Provider-specific tracking
    provider_configurations = {
      postgres = {
        query_log_parsing      = true
        trigger_based_tracking = true
        extension_integration  = "pg_stat_statements"
      }
      kafka = {
        schema_registry_integration = true
        connect_lineage             = true
        streams_topology_tracking   = true
      }
      dagster = {
        asset_lineage       = true
        op_lineage          = true
        dependency_tracking = true
      }
      dbt = {
        manifest_parsing    = true
        compilation_lineage = true
        test_lineage        = true
      }
      s3 = {
        object_versioning_tracking = true
        lifecycle_tracking         = true
        cross_bucket_lineage       = true
      }
    }

    # Lineage inference engine
    inference_engine = {
      enabled = true

      # Pattern-based inference
      pattern_inference = {
        table_name_similarity   = true
        column_name_matching    = true
        data_type_compatibility = true
        cardinality_analysis    = true
      }

      # Statistical inference
      statistical_inference = {
        correlation_analysis      = true
        distribution_matching     = true
        value_overlap_analysis    = true
        temporal_pattern_matching = true
      }

      # ML-based inference
      ml_inference = {
        embedding_similarity       = true
        graph_neural_networks      = true
        transformer_based_matching = true
      }
    }

    # Real-time lineage updates
    real_time_updates = {
      enabled             = true
      change_data_capture = true
      event_sourcing      = true
      streaming_updates   = true

      # Update propagation
      propagation_strategy = "async_batch"
      batch_size           = 500
      max_latency          = "30s"
    }

    # Lineage quality assessment
    quality_assessment = {
      enabled              = true
      completeness_metrics = true
      accuracy_validation  = true
      freshness_monitoring = true

      # Quality scores
      quality_thresholds = {
        completeness = 0.90
        accuracy     = 0.95
        freshness    = "1h"
      }
    }
  }

  # Advanced data quality monitoring
  quality_monitoring {
    enabled = true

    # Multi-dimensional quality metrics
    quality_dimensions = [
      {
        dimension                 = "completeness"
        enabled                   = true
        null_value_analysis       = true
        missing_value_patterns    = true
        required_field_validation = true
      },
      {
        dimension            = "validity"
        enabled              = true
        data_type_validation = true
        format_validation    = true
        range_validation     = true
        regex_validation     = true
      },
      {
        dimension                = "consistency"
        enabled                  = true
        cross_table_consistency  = true
        referential_integrity    = true
        business_rule_validation = true
      },
      {
        dimension            = "accuracy"
        enabled              = true
        statistical_accuracy = true
        benchmark_comparison = true
        external_validation  = true
      },
      {
        dimension            = "timeliness"
        enabled              = true
        freshness_monitoring = true
        latency_tracking     = true
        sla_compliance       = true
      },
      {
        dimension               = "uniqueness"
        enabled                 = true
        duplicate_detection     = true
        primary_key_validation  = true
        business_key_uniqueness = true
      }
    ]

    # Automated quality rules
    automated_rules = {
      enabled = true

      # Statistical profiling rules
      statistical_rules = [
        {
          name             = "outlier_detection"
          method           = "iqr"
          threshold        = 3.0
          apply_to_numeric = true
        },
        {
          name            = "distribution_drift"
          method          = "kolmogorov_smirnov"
          threshold       = 0.05
          baseline_window = "30d"
        },
        {
          name          = "cardinality_monitoring"
          method        = "distinct_count_ratio"
          threshold_min = 0.01
          threshold_max = 0.95
        }
      ]

      # Business rules
      business_rules = [
        {
          name           = "customer_age_validation"
          rule           = "age BETWEEN 18 AND 120"
          table_pattern  = "*customer*"
          column_pattern = "*age*"
        },
        {
          name           = "email_format_validation"
          rule           = "email ~ '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}$'"
          table_pattern  = "*"
          column_pattern = "*email*"
        },
        {
          name           = "future_date_validation"
          rule           = "created_at <= CURRENT_TIMESTAMP"
          table_pattern  = "*"
          column_pattern = "*created*"
        }
      ]
    }

    # Quality scoring
    quality_scoring = {
      enabled = true

      # Weighted scoring system
      dimension_weights = {
        completeness = 0.25
        validity     = 0.25
        consistency  = 0.20
        accuracy     = 0.15
        timeliness   = 0.10
        uniqueness   = 0.05
      }

      # Score aggregation
      aggregation_method        = "weighted_average"
      minimum_score_threshold   = 0.80
      excellent_score_threshold = 0.95
    }

    # Anomaly detection
    anomaly_detection = {
      enabled = true

      # Detection methods
      methods = [
        {
          name          = "statistical_outliers"
          algorithm     = "isolation_forest"
          contamination = 0.05
        },
        {
          name        = "time_series_anomalies"
          algorithm   = "lstm_autoencoder"
          window_size = 168 # 1 week in hours
        },
        {
          name      = "pattern_anomalies"
          algorithm = "one_class_svm"
          kernel    = "rbf"
        }
      ]

      # Anomaly severity classification
      severity_classification = {
        critical = { score_threshold = 0.95, notification = "immediate" }
        high     = { score_threshold = 0.85, notification = "1h" }
        medium   = { score_threshold = 0.70, notification = "4h" }
        low      = { score_threshold = 0.50, notification = "daily" }
      }
    }

    # Alert configuration
    alert_configuration = {
      enabled = true

      # Alert thresholds
      thresholds = {
        completeness = 0.85
        validity     = 0.90
        consistency  = 0.80
        accuracy     = 0.90
        timeliness   = "2h"
        uniqueness   = 0.95
      }

      # Notification channels
      notification_channels = [
        {
          type               = "email"
          recipients         = ["data-quality@company.com", "data-engineering@company.com"]
          severity_threshold = "medium"
        },
        {
          type               = "slack"
          webhook_url        = var.slack_webhook_url
          channel            = "#data-alerts"
          severity_threshold = "high"
        },
        {
          type               = "pagerduty"
          integration_key    = var.pagerduty_key
          severity_threshold = "critical"
        }
      ]

      # Alert aggregation
      aggregation = {
        enabled               = true
        window                = "15m"
        max_alerts_per_window = 10
        digest_schedule       = "hourly"
      }
    }
  }

  # Comprehensive scheduling
  scheduling {
    # Primary discovery runs
    full_discovery_schedule     = "0 2 * * 0"    # Weekly on Sunday at 2 AM
    incremental_scan_schedule   = "0 */6 * * *"  # Every 6 hours
    lineage_refresh_schedule    = "0 4 * * *"    # Daily at 4 AM
    quality_monitoring_schedule = "*/30 * * * *" # Every 30 minutes

    # Performance optimization
    parallel_execution   = true
    max_concurrent_scans = 5
    resource_allocation = {
      cpu_limit      = "4"
      memory_limit   = "8Gi"
      gpu_allocation = "1"
    }

    # Priority scheduling
    priority_queues = {
      critical = { weight = 10, max_concurrent = 2 }
      high     = { weight = 5, max_concurrent = 3 }
      normal   = { weight = 1, max_concurrent = 5 }
    }
  }

  # Enterprise integration
  enterprise_integration = {
    # Catalog integration
    data_catalog_sync = true
    catalog_providers = ["apache_atlas", "datahub", "alation"]

    # Governance integration
    governance_framework_sync      = true
    policy_enforcement_integration = true

    # Workflow integration
    workflow_triggers = {
      new_pii_detected    = "create_governance_ticket"
      quality_degradation = "alert_data_steward"
      lineage_break       = "investigate_data_flow"
    }
  }
}

# =====================================
# ADVANCED DATA LINEAGE CONFIGURATION
# =====================================

# Comprehensive Data Lineage System
create "kolumn_data_lineage" "enterprise_lineage_v2" {
  name    = "Enterprise Data Lineage System v2.0"
  version = "2.0"

  # Multi-dimensional lineage tracking
  tracking_scope = {
    # Infrastructure providers
    providers = [
      "postgres", "mysql", "mssql", "sqlite",
      "kafka", "pulsar", "kinesis",
      "dagster", "prefect", "airflow",
      "s3", "azure_blob", "gcs",
      "snowflake", "bigquery", "redshift", "databricks"
    ]

    # Resource types
    resource_types = [
      "tables", "views", "materialized_views", "functions", "procedures",
      "topics", "streams", "schemas",
      "assets", "ops", "jobs", "workflows",
      "buckets", "objects", "files"
    ]

    # Granularity levels
    granularity_levels = ["database", "schema", "table", "column", "field", "attribute"]

    # Include metadata objects
    include_views              = true
    include_functions          = true
    include_stored_procedures  = true
    include_materialized_views = true
    include_temporary_objects  = false
  }

  # Advanced lineage capture methods
  capture_methods = [
    {
      method    = "query_log_parsing"
      enabled   = true
      providers = ["postgres", "mysql", "mssql"]

      configuration = {
        log_level                = "statement"
        parse_ddl                = true
        parse_dml                = true
        parse_select             = true
        include_temporary_tables = false

        # Performance tuning
        batch_size      = 1000
        parsing_threads = 4
        memory_limit    = "2GB"
      }
    },
    {
      method    = "metadata_api_integration"
      enabled   = true
      providers = ["dagster", "prefect", "airflow"]

      configuration = {
        api_polling_interval         = "5m"
        incremental_updates          = true
        dependency_depth             = 10
        include_runtime_dependencies = true
      }
    },
    {
      method    = "schema_registry_tracking"
      enabled   = true
      providers = ["kafka", "pulsar"]

      configuration = {
        schema_evolution_tracking = true
        compatibility_analysis    = true
        producer_consumer_mapping = true
      }
    },
    {
      method    = "code_analysis"
      enabled   = true
      providers = ["dbt", "spark", "python"]

      configuration = {
        static_analysis            = true
        ast_parsing                = true
        import_dependency_tracking = true
        function_call_analysis     = true
      }
    },
    {
      method    = "runtime_profiling"
      enabled   = true
      providers = ["all"]

      configuration = {
        sampling_rate                = 0.1
        performance_impact_threshold = "5%"
        adaptive_sampling            = true
      }
    }
  ]

  # Lineage relationships and semantics
  relationship_types = [
    {
      type             = "direct_copy"
      description      = "Direct copy without transformation"
      confidence_score = 1.0
      bidirectional    = false
    },
    {
      type                    = "transformed"
      description             = "Data transformation applied"
      confidence_score        = 0.9
      transformation_metadata = true
    },
    {
      type                 = "aggregated"
      description          = "Data aggregation performed"
      confidence_score     = 0.8
      aggregation_metadata = true
    },
    {
      type                    = "joined"
      description             = "Multiple sources joined"
      confidence_score        = 0.7
      join_condition_metadata = true
    },
    {
      type                      = "filtered"
      description               = "Data filtered or subsetted"
      confidence_score          = 0.8
      filter_condition_metadata = true
    },
    {
      type                 = "derived"
      description          = "Derived or calculated field"
      confidence_score     = 0.6
      calculation_metadata = true
    },
    {
      type                      = "inferred"
      description               = "Relationship inferred through analysis"
      confidence_score          = 0.4
      inference_method_metadata = true
    }
  ]

  # Complex lineage rules with advanced pattern matching
  lineage_rules = [
    {
      name              = "customer_data_flow"
      source_pattern    = "postgres:public.customers"
      target_pattern    = "kafka:customer-events"
      relationship_type = "transformed"
      transformation_metadata = {
        type                 = "change_data_capture"
        tool                 = "debezium"
        transformation_logic = "full_row_capture"
      }
      confidence_boost = 0.1
    },
    {
      name              = "analytics_pipeline"
      source_pattern    = "kafka:customer-events"
      target_pattern    = "s3:analytics/customer-data/"
      relationship_type = "aggregated"
      transformation_metadata = {
        type                  = "stream_processing"
        tool                  = "kafka_streams"
        aggregation_window    = "1h"
        aggregation_functions = ["count", "sum", "avg"]
      }
    },
    {
      name              = "ml_feature_engineering"
      source_pattern    = "s3:analytics/customer-data/"
      target_pattern    = "s3:ml-features/customer-features/"
      relationship_type = "derived"
      transformation_metadata = {
        type                    = "feature_engineering"
        tool                    = "python_pandas"
        feature_transformations = ["normalization", "encoding", "binning"]
      }
    },
    {
      name              = "cross_database_replication"
      source_pattern    = "postgres:prod.*"
      target_pattern    = "postgres:analytics.*"
      relationship_type = "direct_copy"
      transformation_metadata = {
        type            = "database_replication"
        tool            = "pg_logical_replication"
        replication_lag = "5s"
      }
    }
  ]

  # Graph-based lineage analysis
  graph_analysis = {
    enabled = true

    # Graph algorithms
    algorithms = [
      {
        name                         = "impact_analysis"
        algorithm                    = "forward_traversal"
        max_depth                    = 15
        include_confidence_weighting = true
      },
      {
        name                      = "root_cause_analysis"
        algorithm                 = "backward_traversal"
        max_depth                 = 10
        include_temporal_analysis = true
      },
      {
        name                      = "critical_path_analysis"
        algorithm                 = "shortest_path"
        weight_by_confidence      = true
        weight_by_usage_frequency = true
      },
      {
        name                       = "circular_dependency_detection"
        algorithm                  = "cycle_detection"
        report_all_cycles          = true
        cycle_breaking_suggestions = true
      },
      {
        name                            = "data_freshness_propagation"
        algorithm                       = "topological_sort"
        calculate_staleness_propagation = true
      }
    ]

    # Graph metrics calculation
    metrics = {
      node_centrality    = true
      edge_importance    = true
      subgraph_density   = true
      component_analysis = true

      # Business metrics
      data_asset_criticality    = true
      usage_frequency_analysis  = true
      downstream_impact_scoring = true
    }

    # Graph optimization
    optimization = {
      graph_compression        = true
      redundant_edge_removal   = true
      confidence_based_pruning = true
      temporal_edge_expiration = true

      # Performance optimization
      graph_partitioning  = true
      incremental_updates = true
      materialized_views  = true
    }
  }

  # Performance and scalability configuration
  performance_optimization = {
    # Caching strategy
    caching = {
      enabled                     = true
      lineage_cache_ttl           = "2h"
      query_result_cache_ttl      = "30m"
      graph_computation_cache_ttl = "4h"

      # Cache distribution
      distributed_cache  = true
      cache_replication  = 2
      cache_partitioning = "consistent_hashing"
    }

    # Incremental processing
    incremental_processing = {
      enabled                 = true
      change_detection_method = "timestamp_based"
      batch_size              = 5000
      processing_window       = "15m"

      # Delta processing
      delta_compression      = true
      delta_storage_format   = "parquet"
      delta_retention_period = "30d"
    }

    # Parallel processing
    parallel_processing = {
      enabled          = true
      worker_pool_size = 8
      async_processing = true
      queue_management = "priority_based"

      # Resource allocation
      cpu_allocation     = "4"
      memory_allocation  = "8Gi"
      storage_allocation = "100Gi"
    }

    # Database optimization
    database_optimization = {
      indexing_strategy = "composite_indexes"
      partitioning      = "time_based"
      compression       = "lz4"

      # Query optimization
      query_optimization     = true
      execution_plan_caching = true
      prepared_statements    = true
    }
  }

  # Real-time lineage updates
  real_time_processing = {
    enabled = true

    # Event streaming
    event_streaming = {
      enabled          = true
      stream_processor = "kafka_streams"
      topic_name       = "lineage-events"

      # Stream processing configuration
      processing_guarantee  = "exactly_once"
      state_store_type      = "rocksdb"
      changelog_replication = 3
    }

    # Change data capture
    change_data_capture = {
      enabled           = true
      cdc_tool          = "debezium"
      source_connectors = ["postgres", "mysql", "mssql"]

      # CDC configuration
      snapshot_mode               = "initial"
      replication_slot_management = "auto"
      schema_evolution_handling   = "adaptive"
    }

    # Event processing rules
    event_processing_rules = [
      {
        event_type = "table_created"
        action     = "register_new_node"
        priority   = "high"
      },
      {
        event_type = "table_dropped"
        action     = "remove_node_and_edges"
        priority   = "high"
      },
      {
        event_type = "data_flow_detected"
        action     = "create_or_update_edge"
        priority   = "medium"
      },
      {
        event_type = "schema_changed"
        action     = "update_node_metadata"
        priority   = "medium"
      }
    ]
  }

  # Advanced visualization and export
  visualization = {
    enabled = true

    # Export formats
    export_formats = ["graphml", "json", "cypher", "gexf", "dot", "svg", "png"]

    # Visualization engines
    visualization_engines = [
      {
        engine           = "d3js"
        layout_algorithm = "force_directed"
        max_nodes        = 1000
        interactive      = true
      },
      {
        engine           = "cytoscape"
        layout_algorithm = "hierarchical"
        max_nodes        = 2000
        styling_support  = true
      },
      {
        engine           = "graphviz"
        layout_algorithm = "dot"
        output_formats   = ["svg", "png", "pdf"]
        batch_generation = true
      }
    ]

    # Visual styling
    styling = {
      node_styling = {
        size_by_importance = true
        color_by_provider  = true
        shape_by_type      = true
        label_truncation   = 20
      }

      edge_styling = {
        thickness_by_confidence    = true
        color_by_relationship_type = true
        arrow_styling              = true
        label_positioning          = "center"
      }

      layout_options = {
        hierarchical_layout = true
        cluster_by_provider = true
        temporal_layout     = true
        physics_simulation  = true
      }
    }

    # Interactive features
    interactive_features = {
      zoom_and_pan         = true
      node_selection       = true
      edge_filtering       = true
      search_functionality = true

      # Advanced interactions
      impact_analysis_mode = true
      path_highlighting    = true
      subgraph_extraction  = true
      temporal_navigation  = true
    }
  }

  # Integration with external systems
  external_integration = {
    # Data catalog integration
    data_catalogs = [
      {
        type               = "apache_atlas"
        endpoint           = var.atlas_endpoint
        authentication     = "kerberos"
        sync_frequency     = "hourly"
        bidirectional_sync = true
      },
      {
        type                = "datahub"
        endpoint            = var.datahub_endpoint
        authentication      = "api_key"
        sync_frequency      = "real_time"
        metadata_enrichment = true
      }
    ]

    # Business intelligence tools
    bi_tools = [
      {
        type              = "tableau"
        server_url        = var.tableau_server
        extract_lineage   = true
        workbook_analysis = true
      },
      {
        type               = "power_bi"
        tenant_id          = var.powerbi_tenant
        extract_lineage    = true
        dashboard_analysis = true
      }
    ]

    # Version control integration
    version_control = {
      git_integration = true
      repository_url  = var.lineage_repo_url
      branch_strategy = "feature_branch"

      # Change tracking
      track_config_changes = true
      track_rule_changes   = true
      approval_workflow    = true
    }
  }

  # Governance and compliance integration
  governance_integration = {
    # Policy enforcement
    policy_enforcement = {
      enabled                = true
      lineage_based_policies = true
      data_flow_restrictions = true
      cross_boundary_alerts  = true
    }

    # Compliance reporting
    compliance_reporting = {
      enabled    = true
      frameworks = ["GDPR", "CCPA", "SOX", "BCBS-239"]

      # Automated compliance checks
      data_residency_validation       = true
      cross_border_data_flow_tracking = true
      retention_policy_enforcement    = true
    }

    # Data stewardship
    data_stewardship = {
      steward_assignment       = true
      lineage_review_workflows = true
      quality_issue_escalation = true

      # Steward notifications
      lineage_change_notifications = true
      quality_degradation_alerts   = true
      policy_violation_alerts      = true
    }
  }
}

# =====================================
# ENTERPRISE RBAC WITH LINEAGE INTEGRATION
# =====================================

# Advanced RBAC Policy with Lineage Awareness
create "kolumn_rbac_policy" "lineage_aware_rbac" {
  name = "Lineage-Aware RBAC Policy"

  # Lineage-based access control
  lineage_based_policies = [
    {
      name                 = "upstream_data_sensitivity_propagation"
      description          = "Propagate sensitivity from upstream data sources"
      rule                 = "IF upstream_classification = 'PII' THEN require_role = 'data_steward'"
      propagation_depth    = 5
      confidence_threshold = 0.8
    },
    {
      name                        = "downstream_impact_awareness"
      description                 = "Consider downstream impact for write operations"
      rule                        = "IF downstream_consumers > 10 THEN require_approval = true"
      impact_analysis_depth       = 3
      critical_consumer_threshold = 5
    },
    {
      name                          = "cross_provider_access_control"
      description                   = "Control access based on cross-provider data flows"
      rule                          = "IF lineage_crosses_providers THEN require_elevated_privileges = true"
      provider_boundary_enforcement = true
    }
  ]

  # Dynamic permission calculation
  dynamic_permissions = {
    enabled = true

    # Permission inheritance through lineage
    permission_inheritance = {
      upstream_inheritance   = true
      downstream_propagation = true
      transitive_permissions = true
      max_inheritance_depth  = 7

      # Inheritance rules
      inheritance_rules = [
        {
          rule        = "read_permission_upstream"
          description = "Inherit read permissions from upstream sources"
          condition   = "confidence > 0.9 AND relationship_type = 'direct_copy'"
        },
        {
          rule        = "sensitive_data_propagation"
          description = "Propagate sensitivity classifications downstream"
          condition   = "classification IN ('PII', 'FINANCIAL') AND relationship_type != 'aggregated'"
        }
      ]
    }

    # Context-aware permissions
    context_aware_permissions = {
      temporal_context       = true
      usage_pattern_context  = true
      data_freshness_context = true
      business_context       = true

      # Contextual rules
      contextual_rules = [
        {
          context             = "business_hours"
          permission_modifier = "allow_elevated"
          condition           = "time BETWEEN '08:00' AND '18:00'"
        },
        {
          context             = "data_freshness"
          permission_modifier = "require_validation"
          condition           = "data_age > '24h'"
        }
      ]
    }
  }

  # Advanced role hierarchy with lineage integration
  role_hierarchy = {
    # Data lineage roles
    data_lineage_curator = {
      permissions                 = ["lineage_read", "lineage_edit", "lineage_validate"]
      lineage_scope               = "enterprise_wide"
      can_approve_lineage_changes = true
    }

    data_lineage_analyst = {
      permissions                 = ["lineage_read", "lineage_analyze"]
      lineage_scope               = "department_specific"
      can_perform_impact_analysis = true
    }

    # Governance roles with lineage awareness
    data_governance_steward = {
      permissions                       = ["governance_policy_enforce", "lineage_based_access_control"]
      can_define_lineage_policies       = true
      can_override_lineage_restrictions = true
    }

    # Cross-functional roles
    data_scientist_with_lineage = {
      base_role                    = "data_scientist"
      lineage_permissions          = ["lineage_read", "feature_lineage_track"]
      can_trace_feature_provenance = true
    }
  }

  # Audit and monitoring with lineage context
  audit_configuration = {
    lineage_aware_auditing = true

    # Enhanced audit events
    audit_events = [
      {
        event                  = "lineage_based_access_granted"
        log_lineage_path       = true
        log_decision_rationale = true
      },
      {
        event                       = "cross_provider_access"
        log_data_flow_path          = true
        log_sensitivity_propagation = true
      },
      {
        event                  = "downstream_impact_analysis"
        log_affected_consumers = true
        log_risk_assessment    = true
      }
    ]

    # Compliance reporting with lineage
    compliance_reporting = {
      lineage_based_compliance = true
      data_flow_documentation  = true
      cross_boundary_tracking  = true

      # Regulatory requirements
      gdpr_lineage_reporting    = true
      ccpa_data_flow_mapping    = true
      sox_data_governance_audit = true
    }
  }
}